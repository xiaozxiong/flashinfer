# 
Use debug module by setting `export FLASHINFER_JIT_VERBOSE=1`.


## JIT ops
Compile source files generated by jinjia which locates in `.cache/flashinfer/89/generated/` directory in JIT manner. Related functions is in `torch.py` and `core.py`. A final `.so` object would created in `.cache/flashinfer/89/cached_ops`.

## BatchPrefillWithPagedKVCacheWrapper

`ADDITIONAL_FUNC_PARAMS` represents additional parameters which would be replaced during compilation.
```cpp
//TODO: run prefill with paged KV cache
void BatchPrefillWithPagedKVCacheRun(at::Tensor float_workspace_buffer,
                                     at::Tensor int_workspace_buffer, at::Tensor plan_info_vec,
                                     at::Tensor q, at::Tensor paged_k_cache,
                                     at::Tensor paged_v_cache, at::Tensor qo_indptr,
                                     at::Tensor paged_kv_indptr, at::Tensor paged_kv_indices,
                                     at::Tensor paged_kv_last_page_len, at::Tensor o,
                                     std::optional<at::Tensor> maybe_lse, int64_t mask_mode_code,
                                     int64_t layout, int64_t window_left ADDITIONAL_FUNC_PARAMS){

// ....                
}
```
### Decide Plan in `scheduler.cuh`

```cpp
template <typename IdType>
inline cudaError_t PrefillPlan(..., PrefillPlanInfo& plan_info, ...){

}
struct PrefillPlanInfo {
  int64_t padded_batch_size;
  int64_t total_num_rows;
  int64_t total_num_rows_offset;
  int64_t cta_tile_q;
  int64_t request_indices_offset;
  int64_t qo_tile_indices_offset;
  int64_t kv_tile_indices_offset;
  int64_t merge_indptr_offset;
  int64_t o_indptr_offset;
  int64_t kv_chunk_size_ptr_offset;
  int64_t v_offset;
  int64_t s_offset;
  int64_t block_valid_mask_offset;
  bool enable_cuda_graph;
  bool split_kv;
  
  //...
}
```


### How to apply custom mask


```cpp
// mask.cuh
enum class MaskMode {
  kNone = 0U,    // No mask
  kCausal = 1U,  // Causal mask
  kCustom = 2U,  // Custom mask
};
```
Generate corresponding parameters with **jinja** for different attention computations. Generated files locate `.cache` directory.
```cpp
// csrc/batch_prefill_customize_config.jinja
struct PagedParams {
  using DTypeQ = DTypeQ;
  using DTypeKV = DTypeKV;
  using DTypeO = DTypeO;
  using IdType = IdType;

  DTypeQ* q;
  paged_kv_t<DTypeKV, IdType> paged_kv;
  IdType* q_indptr;
  DTypeO* o;
  float* lse;
  uint_fastdiv group_size;

  {{ additional_params_decl }}
  uint32_t num_qo_heads;
  IdType q_stride_n;
  IdType q_stride_h;
  int32_t window_left;

  IdType* request_indices;
  IdType* qo_tile_indices;
  IdType* kv_tile_indices;
  IdType* merge_indptr;
  IdType* o_indptr;
  bool* block_valid_mask;
  IdType* kv_chunk_size_ptr;
  uint32_t max_total_num_rows;
  uint32_t* total_num_rows;
  uint32_t padded_batch_size;
  bool partition_kv;

  __host__ __device__ __forceinline__ uint32_t get_qo_len(uint32_t batch_idx) const {
    return q_indptr[batch_idx + 1] - q_indptr[batch_idx];
  }

  __host__ __device__ __forceinline__ uint32_t get_kv_len(uint32_t batch_idx) const {
    return paged_kv.get_length(batch_idx);
  }
};
```

### Logger

```py
logger.info(f"Loading JIT ops: {name}")
```

